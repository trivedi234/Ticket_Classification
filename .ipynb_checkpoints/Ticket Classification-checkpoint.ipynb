{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data\n",
    "\n",
    "For this project the support-ticket-classification data from karozak and team was used (https://privdatastorage.blob.core.windows.net/github/support-tickets-classification/datasets/all_tickets.csv)\n",
    "\n",
    "The microsoft team have acquired the data from Endavas help-desk, representative of a real world data-set of technical suppor-request recieved over a period of a month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/all_tickets.csv https://privdatastorage.blob.core.windows.net/github/support-tickets-classification/datasets/all_tickets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains multiple attributes pertaining to free text support request like category, business-area, impact etc. The goal of the project here is to try and classifiy the category of the request using just the free-text body of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi since recruiter lead permission approve req...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>icon dear please setup icon per icon engineers...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>work experience user hi work experience studen...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>requesting meeting hi please help follow equip...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>re expire days hi ask help update passwords co...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  category\n",
       "0  hi since recruiter lead permission approve req...         4\n",
       "1  icon dear please setup icon per icon engineers...         6\n",
       "2  work experience user hi work experience studen...         5\n",
       "3  requesting meeting hi please help follow equip...         5\n",
       "4  re expire days hi ask help update passwords co...         4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(os.path.join(\"../data/all_tickets.csv\")).drop([\"title\", \"ticket_type\", \"sub_category1\", \"sub_category2\", \"business_service\", \"urgency\", \"impact\"],axis=1)\n",
    "data.groupby(\"category\")[\"category\"].value_counts()\n",
    "\n",
    "plt.hist(df[\"category\"], bins=len(set(df[\"category\"])))\n",
    "plt.ylim((0,10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the data-set has an intrinsic problem of imbalanced classes. Some of the categories have very low population density, evident of th efact that certain request categories are seldom occuring by its nature. This means that this problem cannot be dealt by getting new data.\n",
    "\n",
    "The strategy adopted here is to remove certain categories from the scope of classification based on a heuristiclly decided threshold. The steps below performs the neccesary filtering and randomly shuffles the data to avoid any chances of over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48423, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = data[data.groupby(\"category\")['category'].transform('count') > 100]\n",
    "df = shuffle(df)\n",
    "df = df.reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category  category\n",
       "3         3             137\n",
       "4         4           34061\n",
       "5         5            9634\n",
       "6         6            2628\n",
       "7         7             921\n",
       "8         8             239\n",
       "9         9             191\n",
       "11        11            612\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"category\")[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a raw data-set to begin with, next step is to process and normalize the tex body and convert them into TF-IDF feature vectors. The first step in this process involves generating a vocabulary of top occuring words, and in order to avoid an imbalance, the top-words are selected per class based on the size of the class. e.g. top 40% in class1 + top 40% in class2 .....likewise. Snce the feature vector could be rather large, and create problems in memory handlings, a compression to a 100 dimensional vector using PCA was sought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "cache_dir = os.path.join(\"../data\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "le = LabelEncoder()\n",
    "pca = PCA(n_components = 100)\n",
    "\n",
    "\n",
    "def word_processor(requests):\n",
    "    \n",
    "    # the input is body string, which is processed by removing \n",
    "    #  non-alphanumeric character, converting to lower case, and finally lemmatizing\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", requests.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    result = []\n",
    "    for word in tokens:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        if lemmatized_word not in stopwords_set:\n",
    "                result.append(lemmatized_word)\n",
    "    return result\n",
    "            \n",
    "def get_vocab(raw_data, vocab_fraction, cache_file=\"preprocessed_data.pkl\"):\n",
    "        \n",
    "\n",
    "        vocab_dict = {} #we need the vocab dictionary for a small demo later\n",
    "        vocab = []\n",
    "        \n",
    "        agg_docs = raw_data.groupby(\"category\")[\"body\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "        \n",
    "        for idx, doc in agg_docs.iterrows():\n",
    "            tokens = word_processor(doc[\"body\"])\n",
    "            counts = Counter([t for t in tokens if not t in stopwords_set])\n",
    "            sorted_words = list({k: v for k, v in sorted(counts.items(), key=lambda item: item[1], reverse=True)}.keys())\n",
    "            \n",
    "            #the following steps ensures the balance of the vocabulary\n",
    "            \n",
    "            vocab.extend(sorted_words[:int(vocab_fraction*len(sorted_words))+1])\n",
    "            vocab_dict[str(doc[\"category\"])] = sorted_words[:int(vocab_fraction*len(sorted_words))+1]\n",
    "        \n",
    "        vocab = list(set(vocab))\n",
    "\n",
    "\n",
    "  \n",
    "    return vocab_dict, vocab\n",
    "\n",
    "\n",
    "\n",
    "vocab_dict, vocab = get_vocab(df, 0.4)\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                                     vocabulary=list(set(vocab)), \n",
    "                                     lowercase=True, use_idf=True, \n",
    "                                     stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"body\"]).todense()  \n",
    "X = pca.fit_transform(X)\n",
    "labels = le.fit_transform(df[\"category\"].astype(\"str\").to_list())\n",
    "        \n",
    "with open(os.path.join(cache_dir, \"transformer.pkl\"), \"wb\") as f:\n",
    "        cache_data = pickle.dump(dict(stopwords = stopwords_set, lemmatizer = lemmatizer, \n",
    "                                        encoder = le, vectorizer = vectorizer, pca = pca), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x7f9f7f468908>,\n",
       "  <matplotlib.axis.YTick at 0x7f9f7f4687f0>,\n",
       "  <matplotlib.axis.YTick at 0x7f9f889b8b00>,\n",
       "  <matplotlib.axis.YTick at 0x7f9f87683898>,\n",
       "  <matplotlib.axis.YTick at 0x7f9f876840b8>,\n",
       "  <matplotlib.axis.YTick at 0x7f9f876844e0>,\n",
       "  <matplotlib.axis.YTick at 0x7f9f87684978>,\n",
       "  <matplotlib.axis.YTick at 0x7f9f87685400>],\n",
       " <a list of 8 Text yticklabel objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is a small demonstration of similarity between the classes, based on a normalized metric of common occuring tokens\n",
    "# it generates a symmetric similarity matrix which can help to look-up \n",
    "# if there are chances to combine two classes in case if they are closely related\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "comm_mat = np.zeros((8,8))\n",
    "for idx, cat1 in enumerate(vocab_dict.keys()):\n",
    "    for idy, cat2 in enumerate(vocab_dict.keys()):\n",
    "         comm_mat[idx,idy] = len(set(vocab_dict[cat1]) & set(vocab_dict[cat2]))/min([len(vocab_dict[cat1]), len(vocab_dict[cat2])])\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(comm_mat, interpolation='nearest', cmap=plt.cm.bone)\n",
    "plt.title(\"vocab similarity\", fontsize=30)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(set(labels)))\n",
    "plt.xticks(tick_marks, vocab_dict.keys(), fontsize=15)\n",
    "plt.yticks(tick_marks, vocab_dict.keys(), fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.concat([pd.DataFrame(X), pd.DataFrame(labels)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, df, data, comm_mat, labels, vocab, vocab_dict, le, pca, vectorizer, stopwords_set, nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the transformed input data now, we can go ahead and device the strategy to generate bagging samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import deque\n",
    "from itertools import islice\n",
    "\n",
    "def make_samples(data, n_sample, maj_cat, cut_off):\n",
    "    \n",
    "    # This function generates a list of n_samples, where each sample \n",
    "    # contains a portion (cut_off) of te majority classes (maj_cat)\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    fixed_idx = list(data[~data.iloc[:,-1].isin(maj_cat)].index) # these are the minority rows which are present in all samples \n",
    "    maj_idx = []\n",
    "    for  cat in maj_cat:\n",
    "        maj_idx.append(deque(data[data.iloc[:,-1] == cat].index, \n",
    "                               maxlen = len(data[data.iloc[:,-1] == cat])))\n",
    "    \n",
    "\n",
    "    for s in range(n_sample):\n",
    "        var_idx = []\n",
    "        for i, cat in enumerate(maj_idx):\n",
    "            var_idx.extend(list(islice(cat,0,cut_off)))\n",
    "            maj_idx[i].rotate(cut_off-1) # this step ensures a cycling of the majority class entries\n",
    "        \n",
    "        composite_idx = var_idx + fixed_idx \n",
    "        samples.append(data.iloc[composite_idx, :])\n",
    "    \n",
    "    return samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = make_samples(input_data,3,[4, 5],3000)\n",
    "sample.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "for i in range(3):\n",
    "    train, test = train_test_split(shuffle(samples[i]), test_size = 0.2)\n",
    "    train.to_csv(cache_dir + \"/train{}.csv\".format(str(i+1)),  header=False, index=False)\n",
    "    test.to_csv(cache_dir+\"/test{}.csv\".format(str(i+1)),  header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps below will load all the samples to data to S3. We intend to use each sample-set to train seperate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/capstone_project'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "prefix = 'capstone_project'\n",
    "\n",
    "input_data = sagemaker_session.upload_data(path=cache_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we initiate three seperate estimators using pytorch. Each one of them is a neural network with smilar architecture, but will be trained on a different sample-set simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator1 = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'train_set': \"train1.csv\",\n",
    "                        \"test_set\" : \"test1.csv\"\n",
    "                        \"input_dim\": 100\n",
    "                    })\n",
    "\n",
    "estimator2 = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'train_set': \"train2.csv\",\n",
    "                        \"test_set\" : \"test2.csv\"\n",
    "                        \"input_dim\": 100\n",
    "                    })\n",
    "\n",
    "estimator3 = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'train_set': \"train3.csv\",\n",
    "                        \"test_set\" : \"test3.csv\"\n",
    "                        \"input_dim\": 100\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-26 01:33:38 Starting - Starting the training job...\n",
      "2020-06-26 01:33:41 Starting - Launching requested ML instances.........\n",
      "2020-06-26 01:35:26 Starting - Preparing the instances for training............\n",
      "2020-06-26 01:37:18 Downloading - Downloading input data...\n",
      "2020-06-26 01:37:51 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:12,291 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:12,315 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:15,387 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:15,729 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:15,729 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:15,729 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:15,729 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\u001b[0m\n",
      "\u001b[34mCollecting sklearn (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/7b/01510a6229c2176425bda54d15fba05a4b3df169b87265b008480261d2f9/regex-2020.6.8.tar.gz (690kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn (from sklearn->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/42/ec/32310181e803f5d22e0dd33eb18924489b2f8d08cf5b6e116a93a6a5d1c6/scikit_learn-0.22.2.post1-cp35-cp35m-manylinux1_x86_64.whl (7.0MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas->-r requirements.txt (line 1)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting scipy>=0.17.0 (from scikit-learn->sklearn->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/c1/60/8cbf00c0deb50a971e6e3a015fb32513960a92867df979870a454481817c/scipy-1.4.1-cp35-cp35m-manylinux1_x86_64.whl (26.0MB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, sklearn, train, regex\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\u001b[0m\n",
      "\n",
      "2020-06-26 01:38:11 Training - Training image download completed. Training in progress.\u001b[34m  Running setup.py bdist_wheel for sklearn: started\n",
      "  Running setup.py bdist_wheel for sklearn: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vtivhj6p/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/9c/e2/cf/246ad8c87bcdf3cba1ec95fa89bc205c9037aa8f4d2e26fdad\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk sklearn train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, pytz, pandas, joblib, regex, tqdm, nltk, scipy, scikit-learn, sklearn, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed joblib-0.14.1 nltk-3.5 numpy-1.18.5 pandas-0.24.2 pytz-2020.1 regex-2020.6.8 scikit-learn-0.22.2.post1 scipy-1.4.1 sklearn-0.0 tqdm-4.46.1 train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.2b1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:43,843 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-080049645597/sagemaker-pytorch-2020-06-26-01-33-37-598/source/sourcedir.tar.gz\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"log_level\": 20,\n",
      "    \"num_gpus\": 1,\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"hyperparameters\": {\n",
      "        \"input_dim\": 100,\n",
      "        \"train_set\": \"train1.csv\",\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-06-26-01-33-37-598\",\n",
      "    \"current_host\": \"algo-1\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-080049645597/sagemaker-pytorch-2020-06-26-01-33-37-598/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_SET=train1.csv\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--input_dim\",\"100\",\"--train_set\",\"train1.csv\"]\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"input_dim\":100,\"train_set\":\"train1.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-06-26-01-33-37-598\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-080049645597/sagemaker-pytorch-2020-06-26-01-33-37-598/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"input_dim\":100,\"train_set\":\"train1.csv\"}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=100\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --input_dim 100 --train_set train1.csv\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mModel loaded with input_dim 100, hidden_dim1 128, hidden_dim2 64, output_dim\u001b[0m\n",
      "\u001b[34mEpoch: 1, NLLLoss: 2.811637553729509\u001b[0m\n",
      "\u001b[34mEpoch: 2, NLLLoss: 1.5130568146705627\u001b[0m\n",
      "\u001b[34mEpoch: 3, NLLLoss: 1.2375313812180568\u001b[0m\n",
      "\u001b[34mEpoch: 4, NLLLoss: 1.190608190862756\u001b[0m\n",
      "\u001b[34mEpoch: 5, NLLLoss: 1.1554957487081225\u001b[0m\n",
      "\u001b[34mEpoch: 6, NLLLoss: 1.1250597269911515\u001b[0m\n",
      "\u001b[34mEpoch: 7, NLLLoss: 1.1021399435244108\u001b[0m\n",
      "\u001b[34mEpoch: 8, NLLLoss: 1.083566595849238\u001b[0m\n",
      "\u001b[34mEpoch: 9, NLLLoss: 1.065936501873167\u001b[0m\n",
      "\u001b[34mEpoch: 10, NLLLoss: 1.0492077791377117\u001b[0m\n",
      "\u001b[34m2020-06-26 01:38:56,884 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-06-26 01:39:06 Uploading - Uploading generated training model\n",
      "2020-06-26 01:39:06 Completed - Training job completed\n",
      "Training seconds: 108\n",
      "Billable seconds: 108\n"
     ]
    }
   ],
   "source": [
    "estimator1.fit({'training': input_data})\n",
    "estimator2.fit({'training': input_data})\n",
    "estimator3.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps below initiates a string predictor class, and deploys the three estimators on three different endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint sagemaker-pytorch-2020-06-26-02-13-08-579: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e0a54706e07d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                      \u001b[0msource_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'serve'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                      predictor_cls=StringPredictor)\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait, data_capture_config)\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             )\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2402\u001b[0m         )\n\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 ),\n\u001b[1;32m   2660\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m             )\n\u001b[1;32m   2663\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint sagemaker-pytorch-2020-06-26-02-13-08-579: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model1 = PyTorchModel(model_data=estimator1.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor1 = model1.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "model2 = PyTorchModel(model_data=estimator2.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor2 = model2.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "model3 = PyTorchModel(model_data=estimator3.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor3 = model3.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below pings on one of the endpoint and test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test = df.[0:10,:]\n",
    "\n",
    "def test_requests():\n",
    "    \n",
    "    ground = []\n",
    "    result = []\n",
    "    \n",
    "    for idx, row in test.iterrows():\n",
    "        ground.append(int(row[\"category\"]))\n",
    "        reques_input = row[\"body\"].encode(\"utf8\")\n",
    "        result.append(int(predictor.predict(request_input)))\n",
    "        \n",
    "    return ground, result\n",
    "\n",
    "ground, result= test_requests()\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoints are deployed and tested we are ready to create a web API, that would route the incoming request to all the three endpoints and return the maximum voted result as an informative text string e.g. \"your request is forwarded to team-\"category\"\n",
    "\n",
    "To begin with, a lambda function is created using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "from collections import Counter \n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoints, sending the request body, \n",
    "    #and recieving seperate response from all the estimators\n",
    "    response1 = runtime.invoke_endpoint(EndpointName = '**ENDPOINT FOR ESTIMATOR-1**',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "    \n",
    "    response2 = runtime.invoke_endpoint(EndpointName = '**ENDPOINT FOR ESTIMATOR-2**',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "    \n",
    "    response3 = runtime.invoke_endpoint(EndpointName = '**ENDPOINT FOR ESTIMATOR-3**',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result =[]\n",
    "    for res in [response1, response2, response3]:\n",
    "        result.append(res['Body'].read().decode('utf-8'))\n",
    "    \n",
    "    result = Counter(result).most_common(1)[0]\n",
    "    \n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : \"Your reques has been forwarded to team{}\".format(result)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the lambda funbction is setup the final step is to create an API Gateway, and update the public url in the webpage code (**API GATEWAY ENDPOINT HERE**). Once this is complete, the we page can be actively used to submit requests and get an appropriate response displayed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
